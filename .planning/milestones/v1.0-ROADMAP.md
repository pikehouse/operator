# Milestone v1.0: MVP â€” AI-Powered Operator for Distributed Systems

**Status:** SHIPPED 2026-01-25
**Phases:** 1-6
**Total Plans:** 22

## Overview

Build an AI-powered operator that monitors TiKV clusters, diagnoses issues using Claude, and demonstrates real diagnostic reasoning about distributed systems. The operator is service-agnostic with TiKV as the first subject. v1.0 is observe-only (diagnosis and recommendations, no automated action execution).

## Phases

### Phase 1: Foundation

**Goal**: Core abstractions and local deployment infrastructure are ready for subject implementation.
**Depends on**: None
**Plans**: 4 plans

Plans:
- [x] 01-01: Project setup with uv workspace and operator-core package
- [x] 01-02: Subject adapter interface (CORE-01) with Protocol and types
- [x] 01-03: Deployment abstraction (DEPLOY-01) with LocalDeployment
- [x] 01-04: CLI integration (DEPLOY-02) with deploy commands

**Details:**
Requirements covered: CORE-01, DEPLOY-01, DEPLOY-02

Success criteria achieved:
1. A new subject can be added by implementing the adapter interface without modifying core code
2. Local deployment spins up containers via a single command (`operator deploy local up`)
3. Deployment abstraction allows swapping local for cloud without changing operator code

---

### Phase 2: TiKV Subject

**Goal**: Operator can observe TiKV cluster state through a complete subject implementation.
**Depends on**: Phase 1
**Plans**: 5 plans

Plans:
- [x] 02-01: Package setup with operator-tikv and Pydantic response types
- [x] 02-02: PD API client (TDD) for cluster state observation
- [x] 02-03: Prometheus metrics client (TDD) for performance metrics
- [x] 02-04: Log parser (TDD) for leadership change extraction
- [x] 02-05: TiKV invariants and TiKVSubject implementation

**Details:**
Requirements covered: TIKV-01, TIKV-02, TIKV-03, TIKV-04

Success criteria achieved:
1. Operator retrieves cluster topology, region distribution, and store health from PD API
2. Operator queries Prometheus for real-time metrics (QPS, latency, disk usage, Raft lag)
3. TiKV invariants detect when a store is down, latency exceeds threshold, or replication is degraded
4. Log parser extracts leader election events, snapshot transfers, and slow operations from TiKV logs

---

### Phase 3: Local Cluster

**Goal**: Fully containerized test environment with TiKV cluster, observability, and load generation.
**Depends on**: Phase 1, Phase 2
**Plans**: 4 plans

Plans:
- [x] 03-01: TiKV/PD cluster docker-compose (ENV-01)
- [x] 03-02: Prometheus and Grafana observability (ENV-02)
- [x] 03-03: go-ycsb load generator (ENV-03)
- [x] 03-04: Operator container with verification (ENV-04)

**Details:**
Requirements covered: ENV-01, ENV-02, ENV-03, ENV-04

Success criteria achieved:
1. `docker compose up` starts a 6-node TiKV/PD cluster with networking configured
2. Prometheus scrapes all TiKV and PD metrics; Grafana dashboards show cluster health
3. Load generator produces configurable traffic patterns against the cluster
4. Operator container connects to cluster and observability stack without host dependencies

---

### Phase 4: Monitor Loop

**Goal**: Automated invariant checking runs continuously and creates tickets on violations.
**Depends on**: Phase 2, Phase 3
**Plans**: 3 plans

Plans:
- [x] 04-01: Ticket database with SQLite persistence and deduplication
- [x] 04-02: MonitorLoop daemon with signal handling and heartbeat
- [x] 04-03: CLI commands for tickets and monitor daemon

**Details:**
Requirements covered: CORE-02, CORE-03

Success criteria achieved:
1. Tickets persist in SQLite with status transitions (created -> diagnosed -> resolved)
2. Monitor loop runs at configurable intervals (e.g., every 30s) checking all registered invariants
3. When an invariant fails, a ticket is automatically created with the violation details

---

### Phase 5: AI Diagnosis

**Goal**: Claude analyzes tickets and produces structured reasoning about distributed system issues.
**Depends on**: Phase 4
**Plans**: 4 plans

Plans:
- [x] 05-01: Anthropic SDK setup, DiagnosisOutput model, TicketDB update
- [x] 05-02: Context gatherer and prompt builder
- [x] 05-03: AgentRunner daemon with Claude API integration
- [x] 05-04: CLI agent commands and verification

**Details:**
Requirements covered: CORE-04, DIAG-01, DIAG-02, DIAG-03, DIAG-04

Success criteria achieved:
1. Agent picks up undiagnosed tickets and invokes Claude with relevant context
2. Diagnosis output includes observation summary, identified root cause, and supporting evidence
3. AI correlates multiple metrics (e.g., latency + Raft lag + disk I/O) to pinpoint issues
4. Diagnosis logs show alternatives considered (e.g., "could be disk I/O, but metrics don't support")
5. Each diagnosis includes recommended action with rationale (even though v1 is observe-only)

---

### Phase 6: Chaos Demo

**Goal**: End-to-end demonstration showing AI diagnosis of injected faults.
**Depends on**: Phase 5
**Plans**: 2 plans

Plans:
- [x] 06-01: ChaosDemo orchestrator class with Rich output
- [x] 06-02: CLI demo commands and verification

**Details:**
Requirements covered: CHAOS-01

Success criteria achieved:
1. Operator detects store failure within configurable window after node kill
2. AI diagnosis correctly identifies "node X is down" and correlates with missing heartbeats
3. Demo script runs full cycle: healthy cluster -> inject fault -> detect -> diagnose -> explain

---

## Milestone Summary

**Key Decisions:**
- Use workspace source config for automatic package installation
- Protocol-based abstractions for Subject and DeploymentTarget
- Pydantic for API response types, dataclass for internal types
- aiosqlite for non-blocking database operations
- Sequential ticket processing to avoid Claude API rate limits
- Active invariant checking during chaos demo detection

**Issues Resolved:**
- Circular import cycle in monitor module (lazy __getattr__ pattern)
- Port binding dict access in python-on-whales
- Passive polling bug in chaos demo detection

**Technical Debt:**
- Hotspot filtering TODO (subject.py:184) - returns all regions
- Log tail fetching stubbed - deferred to future
- Action implementations raise NotImplementedError - v1 observe-only

---

*For current project status, see .planning/MILESTONES.md*

---
*Archived: 2026-01-25 as part of v1.0 milestone completion*
