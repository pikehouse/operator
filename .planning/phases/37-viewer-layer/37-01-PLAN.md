---
phase: 37-viewer-layer
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - eval/src/eval/cli.py
  - eval/src/eval/runner/db.py
autonomous: true

must_haves:
  truths:
    - "Developer can run `eval list` and see all campaigns in a table"
    - "Developer can run `eval show <campaign_id>` and see campaign details with trial list"
    - "Developer can run `eval show <trial_id>` and see full trial detail with commands"
    - "All commands support --json flag for machine-readable output"
  artifacts:
    - path: "eval/src/eval/cli.py"
      provides: "list and show CLI commands"
      contains: "def list_campaigns"
    - path: "eval/src/eval/runner/db.py"
      provides: "Database queries for listing and getting trials"
      contains: "get_all_campaigns"
  key_links:
    - from: "eval/src/eval/cli.py"
      to: "eval/src/eval/runner/db.py"
      via: "db.get_all_campaigns(), db.get_trial()"
      pattern: "db\\.get_(all_campaigns|trial)"
---

<objective>
Add CLI commands for browsing evaluation data: `eval list` shows all campaigns, `eval show <id>` displays campaign or trial details.

Purpose: Enables developers to quickly browse evaluation results from the command line without needing to query the database directly.

Output: Three CLI commands (list, show campaign, show trial) with plain text and JSON output modes.
</objective>

<execution_context>
@/Users/jrtipton/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jrtipton/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/37-viewer-layer/37-CONTEXT.md
@.planning/phases/37-viewer-layer/37-RESEARCH.md

# Existing code
@eval/src/eval/cli.py
@eval/src/eval/runner/db.py
@eval/src/eval/types.py
@eval/src/eval/analysis/types.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add database query methods for listing and trial retrieval</name>
  <files>eval/src/eval/runner/db.py</files>
  <action>
Add three methods to EvalDB class, following the existing patterns in get_campaign and get_trials:

1. `async def get_all_campaigns(self, limit: int = 100, offset: int = 0) -> list[Campaign]:`
   - Query: `SELECT * FROM campaigns ORDER BY created_at DESC LIMIT ? OFFSET ?`
   - Use `db.row_factory = aiosqlite.Row` like get_campaign does
   - Return list of Campaign objects using same mapping pattern as get_campaign:
     ```python
     return [
         Campaign(
             id=row["id"],
             subject_name=row["subject_name"],
             chaos_type=row["chaos_type"],
             trial_count=row["trial_count"],
             baseline=bool(row["baseline"]),
             created_at=row["created_at"],
         )
         for row in rows
     ]
     ```

2. `async def get_trial(self, trial_id: int) -> Trial | None:`
   - Query: `SELECT * FROM trials WHERE id = ?`
   - Use `db.row_factory = aiosqlite.Row`
   - Return single Trial using same mapping pattern as get_trials, or None if not found:
     ```python
     if row:
         return Trial(
             id=row["id"],
             campaign_id=row["campaign_id"],
             started_at=row["started_at"],
             chaos_injected_at=row["chaos_injected_at"],
             ticket_created_at=row["ticket_created_at"],
             resolved_at=row["resolved_at"],
             ended_at=row["ended_at"],
             initial_state=row["initial_state"],
             final_state=row["final_state"],
             chaos_metadata=row["chaos_metadata"],
             commands_json=row["commands_json"],
         )
     return None
     ```

3. `async def count_campaigns(self) -> int:`
   - Query: `SELECT COUNT(*) FROM campaigns`
   - Use `cursor.fetchone()` and return `row[0]` (integer count)
  </action>
  <verify>
Run:
```bash
cd eval && python -c "
from eval.runner.db import EvalDB
db = EvalDB.__new__(EvalDB)
assert hasattr(db, 'get_all_campaigns'), 'Missing get_all_campaigns'
assert hasattr(db, 'get_trial'), 'Missing get_trial'
assert hasattr(db, 'count_campaigns'), 'Missing count_campaigns'
print('OK: All methods exist')
"
```
  </verify>
  <done>EvalDB has get_all_campaigns, get_trial, and count_campaigns methods with correct signatures.</done>
</task>

<task type="auto">
  <name>Task 2: Add eval list command</name>
  <files>eval/src/eval/cli.py</files>
  <action>
Add `list` command to display campaigns in a compact table. Follow the existing pattern in cli.py (async def run() inside command, asyncio.run(run())).

```python
@app.command("list")
def list_campaigns(
    db_path: Path = typer.Option(Path("eval.db"), "--db", help="Path to eval database"),
    limit: int = typer.Option(20, "--limit", "-n", help="Number of campaigns to show"),
    offset: int = typer.Option(0, "--offset", help="Skip first N campaigns"),
    json_output: bool = typer.Option(False, "--json", help="Output as JSON"),
) -> None:
    """List all campaigns in the database."""
    async def run():
        db = EvalDB(db_path)
        await db.ensure_schema()
        campaigns = await db.get_all_campaigns(limit=limit, offset=offset)
        total = await db.count_campaigns()
        return campaigns, total

    campaigns, total = asyncio.run(run())

    if json_output:
        # Output JSON array with keys: id, subject_name, chaos_type, trial_count, baseline, created_at
        import json
        data = [
            {
                "id": c.id,
                "subject_name": c.subject_name,
                "chaos_type": c.chaos_type,
                "trial_count": c.trial_count,
                "baseline": c.baseline,
                "created_at": c.created_at,
            }
            for c in campaigns
        ]
        print(json.dumps(data, indent=2))
        return

    # Plain text table with fixed column widths (no Rich tables)
    # Handle empty database case
    if not campaigns:
        print("No campaigns found.")
        print(f"Database: {db_path}")
        return

    # Header row with fixed widths: ID(6), Date(12), Subject(10), Chaos(12), Trials(8), Baseline(8)
    print(f"{'ID':<6} {'Date':<12} {'Subject':<10} {'Chaos':<12} {'Trials':<8} {'Baseline':<8}")
    print("-" * 58)
    for c in campaigns:
        date_str = c.created_at[:10] if c.created_at else "N/A"
        baseline_str = "Yes" if c.baseline else "No"
        print(f"{c.id:<6} {date_str:<12} {c.subject_name:<10} {c.chaos_type:<12} {c.trial_count:<8} {baseline_str:<8}")

    # Show pagination info
    showing_end = min(offset + limit, total)
    print(f"\nShowing {offset + 1}-{showing_end} of {total} campaigns")
```

Note: Import `json` at top of file if not already present.
  </action>
  <verify>
Run `cd eval && uv run eval list --help` to verify command exists and shows options.
  </verify>
  <done>eval list command shows campaigns in table format with --json option and handles empty database.</done>
</task>

<task type="auto">
  <name>Task 3: Add eval show command for campaigns and trials</name>
  <files>eval/src/eval/cli.py</files>
  <action>
Add `show` command that handles both campaign IDs and trial IDs. Follow the existing async pattern.

```python
@app.command("show")
def show_detail(
    id: int = typer.Argument(..., help="Campaign or trial ID"),
    trial: bool = typer.Option(False, "--trial", "-t", help="Treat ID as trial ID"),
    db_path: Path = typer.Option(Path("eval.db"), "--db", help="Path to eval database"),
    json_output: bool = typer.Option(False, "--json", help="Output as JSON"),
) -> None:
    """Show details for a campaign or trial.

    By default, treats ID as a campaign ID. Use --trial flag for trial ID.

    Examples:
        eval show 1              # Show campaign 1
        eval show --trial 5      # Show trial 5
    """
    import json

    async def run():
        db = EvalDB(db_path)
        await db.ensure_schema()

        if trial:
            # Fetch trial by ID
            t = await db.get_trial(id)
            if t is None:
                console.print(f"[red]Error: Trial {id} not found[/red]")
                raise typer.Exit(1)
            return ("trial", t, None, None)
        else:
            # Fetch campaign and its trials
            campaign = await db.get_campaign(id)
            if campaign is None:
                console.print(f"[red]Error: Campaign {id} not found[/red]")
                raise typer.Exit(1)
            trials = await db.get_trials(id)

            # Get campaign analysis for aggregate scores
            from eval.analysis import analyze_campaign
            try:
                summary = await analyze_campaign(db, id)
            except ValueError:
                summary = None
            return ("campaign", campaign, trials, summary)

    result_type, obj, trials, summary = asyncio.run(run())

    if result_type == "trial":
        # Trial detail output
        t = obj
        if json_output:
            # Parse commands_json for output
            commands = json.loads(t.commands_json) if t.commands_json else []
            data = {
                "id": t.id,
                "campaign_id": t.campaign_id,
                "started_at": t.started_at,
                "chaos_injected_at": t.chaos_injected_at,
                "ticket_created_at": t.ticket_created_at,
                "resolved_at": t.resolved_at,
                "ended_at": t.ended_at,
                "initial_state": json.loads(t.initial_state) if t.initial_state else None,
                "final_state": json.loads(t.final_state) if t.final_state else None,
                "chaos_metadata": json.loads(t.chaos_metadata) if t.chaos_metadata else None,
                "commands": commands,
            }
            print(json.dumps(data, indent=2))
            return

        # Plain text trial detail
        print(f"Trial {t.id} (Campaign {t.campaign_id})")
        print("-" * 40)
        print(f"Started:        {t.started_at}")
        print(f"Chaos injected: {t.chaos_injected_at}")
        if t.ticket_created_at:
            print(f"Ticket created: {t.ticket_created_at}")
        if t.resolved_at:
            print(f"Resolved:       {t.resolved_at}")
        print(f"Ended:          {t.ended_at}")
        print()

        # Commands list
        commands = json.loads(t.commands_json) if t.commands_json else []
        if commands:
            print("Commands:")
            for i, cmd in enumerate(commands, 1):
                # Each command is a string or dict with 'command' key
                cmd_str = cmd if isinstance(cmd, str) else cmd.get("command", str(cmd))
                # Indent and truncate long commands
                cmd_display = cmd_str[:80] + "..." if len(cmd_str) > 80 else cmd_str
                print(f"  {i}. {cmd_display}")
        else:
            print("Commands: (none recorded)")
        print()

        # Show initial/final state summary
        print("States (use --json for full detail):")
        print(f"  Initial: {len(t.initial_state)} bytes")
        print(f"  Final:   {len(t.final_state)} bytes")

    else:
        # Campaign detail output
        campaign = obj
        if json_output:
            data = {
                "id": campaign.id,
                "subject_name": campaign.subject_name,
                "chaos_type": campaign.chaos_type,
                "trial_count": campaign.trial_count,
                "baseline": campaign.baseline,
                "created_at": campaign.created_at,
                "trials": [
                    {
                        "id": t.id,
                        "started_at": t.started_at,
                        "resolved_at": t.resolved_at,
                        "ended_at": t.ended_at,
                    }
                    for t in trials
                ],
            }
            if summary:
                data["win_rate"] = summary.win_rate
                data["success_count"] = summary.success_count
                data["failure_count"] = summary.failure_count
                data["timeout_count"] = summary.timeout_count
            print(json.dumps(data, indent=2))
            return

        # Plain text campaign detail
        print(f"Campaign {campaign.id}: {campaign.subject_name}/{campaign.chaos_type}")
        print("-" * 50)
        print(f"Created:  {campaign.created_at}")
        print(f"Trials:   {campaign.trial_count}")
        print(f"Baseline: {'Yes' if campaign.baseline else 'No'}")
        print()

        # Aggregate scores if available
        if summary:
            print("Scores:")
            print(f"  Win rate:   {summary.win_rate:.1%}")
            print(f"  Success:    {summary.success_count}")
            print(f"  Failure:    {summary.failure_count}")
            print(f"  Timeout:    {summary.timeout_count}")
            if summary.avg_time_to_detect_sec:
                print(f"  Avg detect: {summary.avg_time_to_detect_sec:.1f}s")
            if summary.avg_time_to_resolve_sec:
                print(f"  Avg resolve: {summary.avg_time_to_resolve_sec:.1f}s")
            print()

        # Trial list table
        if trials:
            print("Trials:")
            # Header: ID(6), Started(20), Resolved(20), Status
            print(f"  {'ID':<6} {'Started':<20} {'Resolved':<20}")
            for t in trials:
                resolved_str = t.resolved_at[:19] if t.resolved_at else "N/A"
                started_str = t.started_at[:19] if t.started_at else "N/A"
                print(f"  {t.id:<6} {started_str:<20} {resolved_str:<20}")
        else:
            print("No trials recorded.")
```
  </action>
  <verify>
Run `cd eval && uv run eval show --help` to verify command accepts ID argument and --trial flag.
  </verify>
  <done>eval show <id> displays campaign detail with aggregate scores and trial list, eval show --trial <id> displays trial detail with commands.</done>
</task>

</tasks>

<verification>
1. `cd eval && uv run eval list` - Shows empty table or campaigns if eval.db exists
2. `cd eval && uv run eval list --json` - Returns JSON array
3. `cd eval && uv run eval show 1` - Shows campaign detail (or error if not found)
4. `cd eval && uv run eval show --trial 1` - Shows trial detail (or error if not found)
5. All commands work with --db flag for custom database path
</verification>

<success_criteria>
- VIEW-01 satisfied: eval list shows campaigns
- VIEW-02 satisfied: eval show <campaign_id> displays campaign + trials
- VIEW-03 satisfied: eval show --trial <trial_id> displays single trial detail
- All commands have --json flag for machine-readable output
- Plain text output is compact and works in any terminal (no colors)
</success_criteria>

<output>
After completion, create `.planning/phases/37-viewer-layer/37-01-SUMMARY.md`
</output>
