---
phase: 37-viewer-layer
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - eval/src/eval/cli.py
  - eval/src/eval/runner/db.py
autonomous: true

must_haves:
  truths:
    - "Developer can run `eval list` and see all campaigns in a table"
    - "Developer can run `eval show <campaign_id>` and see campaign details with trial list"
    - "Developer can run `eval show <trial_id>` and see full trial detail with commands"
    - "All commands support --json flag for machine-readable output"
  artifacts:
    - path: "eval/src/eval/cli.py"
      provides: "list and show CLI commands"
      contains: "def list_campaigns"
    - path: "eval/src/eval/runner/db.py"
      provides: "Database queries for listing and getting trials"
      contains: "get_all_campaigns"
  key_links:
    - from: "eval/src/eval/cli.py"
      to: "eval/src/eval/runner/db.py"
      via: "db.get_all_campaigns(), db.get_trial()"
      pattern: "db\\.get_(all_campaigns|trial)"
---

<objective>
Add CLI commands for browsing evaluation data: `eval list` shows all campaigns, `eval show <id>` displays campaign or trial details.

Purpose: Enables developers to quickly browse evaluation results from the command line without needing to query the database directly.

Output: Three CLI commands (list, show campaign, show trial) with plain text and JSON output modes.
</objective>

<execution_context>
@/Users/jrtipton/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jrtipton/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/37-viewer-layer/37-CONTEXT.md
@.planning/phases/37-viewer-layer/37-RESEARCH.md

# Existing code
@eval/src/eval/cli.py
@eval/src/eval/runner/db.py
@eval/src/eval/types.py
@eval/src/eval/analysis/types.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add database query methods for listing and trial retrieval</name>
  <files>eval/src/eval/runner/db.py</files>
  <action>
Add three methods to EvalDB class:

1. `get_all_campaigns(limit: int = 100, offset: int = 0) -> list[Campaign]`
   - Query: SELECT * FROM campaigns ORDER BY created_at DESC LIMIT ? OFFSET ?
   - Returns list of Campaign objects

2. `get_trial(trial_id: int) -> Trial | None`
   - Query: SELECT * FROM trials WHERE id = ?
   - Returns single Trial or None

3. `count_campaigns() -> int`
   - Query: SELECT COUNT(*) FROM campaigns
   - Returns total count for pagination info

Use existing get_campaign and get_trials patterns for consistency.
  </action>
  <verify>
Run `python -c "from eval.runner.db import EvalDB; print('OK')"` to verify import succeeds.
  </verify>
  <done>EvalDB has get_all_campaigns, get_trial, and count_campaigns methods.</done>
</task>

<task type="auto">
  <name>Task 2: Add eval list command</name>
  <files>eval/src/eval/cli.py</files>
  <action>
Add `list` command to display campaigns in a compact table:

```python
@app.command("list")
def list_campaigns(
    db_path: Path = typer.Option(Path("eval.db"), "--db", help="Path to eval database"),
    limit: int = typer.Option(20, "--limit", "-n", help="Number of campaigns to show"),
    offset: int = typer.Option(0, "--offset", help="Skip first N campaigns"),
    json_output: bool = typer.Option(False, "--json", help="Output as JSON"),
) -> None:
```

Plain text output format (compact, one line per campaign):
```
ID  Date        Subject  Chaos       Trials  Win Rate
1   2026-01-29  tikv     node_kill   5       80.0%
2   2026-01-29  tikv     node_kill   3       66.7%
```

Use manual string formatting with fixed widths (not Rich tables) for plain text.
For --json, output list of dicts with keys: id, subject_name, chaos_type, trial_count, baseline, created_at.

Pattern: async def run() inside command, asyncio.run(run()).
  </action>
  <verify>
Run `cd eval && uv run eval list --help` to verify command exists.
  </verify>
  <done>eval list command shows campaigns in table format with --json option.</done>
</task>

<task type="auto">
  <name>Task 3: Add eval show command for campaigns and trials</name>
  <files>eval/src/eval/cli.py</files>
  <action>
Add `show` command that handles both campaign IDs and trial IDs:

```python
@app.command("show")
def show_detail(
    id: int = typer.Argument(..., help="Campaign or trial ID"),
    trial: bool = typer.Option(False, "--trial", "-t", help="Treat ID as trial ID"),
    db_path: Path = typer.Option(Path("eval.db"), "--db", help="Path to eval database"),
    json_output: bool = typer.Option(False, "--json", help="Output as JSON"),
) -> None:
```

**Campaign detail (default):**
- Show campaign metadata (subject, chaos, baseline, created_at)
- Show aggregate scores (call analyze_campaign from analysis module)
- List all trials in table format: ID, Started, Outcome, Detection, Resolution

**Trial detail (--trial flag):**
- Show trial metadata (campaign_id, started_at, ended_at, etc.)
- Show timing breakdown (time to detect, time to resolve)
- Show commands as indented list (parse commands_json)
- Show initial/final state (JSON pretty-printed)

Plain text uses manual formatting. JSON uses Pydantic serialization where available.

Handle not-found cases with clear error messages and typer.Exit(1).
  </action>
  <verify>
Run `cd eval && uv run eval show --help` to verify command accepts ID argument and --trial flag.
  </verify>
  <done>eval show <id> displays campaign detail, eval show --trial <id> displays trial detail.</done>
</task>

</tasks>

<verification>
1. `cd eval && uv run eval list` - Shows empty table or campaigns if eval.db exists
2. `cd eval && uv run eval list --json` - Returns JSON array
3. `cd eval && uv run eval show 1` - Shows campaign detail (or error if not found)
4. `cd eval && uv run eval show --trial 1` - Shows trial detail (or error if not found)
5. All commands work with --db flag for custom database path
</verification>

<success_criteria>
- VIEW-01 satisfied: eval list shows campaigns
- VIEW-02 satisfied: eval show <campaign_id> displays campaign + trials
- VIEW-03 satisfied: eval show --trial <trial_id> displays single trial detail
- All commands have --json flag for machine-readable output
- Plain text output is compact and works in any terminal (no colors)
</success_criteria>

<output>
After completion, create `.planning/phases/37-viewer-layer/37-01-SUMMARY.md`
</output>
