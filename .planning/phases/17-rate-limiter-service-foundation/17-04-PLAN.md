---
phase: 17-rate-limiter-service-foundation
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/ratelimiter-service/src/ratelimiter_service/api/rate_limit.py
  - packages/ratelimiter-service/src/ratelimiter_service/api/management.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "CHECK_LATENCY histogram records request latency"
    - "ACTIVE_COUNTERS gauge reflects current counter count"
  artifacts:
    - path: "packages/ratelimiter-service/src/ratelimiter_service/api/rate_limit.py"
      provides: "Latency recording for rate limit checks"
      contains: "CHECK_LATENCY"
    - path: "packages/ratelimiter-service/src/ratelimiter_service/api/management.py"
      provides: "Active counters gauge update"
      contains: "set_active_counters"
  key_links:
    - from: "api/rate_limit.py"
      to: "metrics.py"
      via: "CHECK_LATENCY.time() decorator"
      pattern: "CHECK_LATENCY\\.time\\(\\)"
    - from: "api/management.py"
      to: "metrics.py"
      via: "set_active_counters call"
      pattern: "set_active_counters\\(len\\(counters\\)\\)"
---

<objective>
Wire up unused Prometheus metrics to complete Phase 17 success criterion 4.

Purpose: The verification found that CHECK_LATENCY histogram and ACTIVE_COUNTERS gauge are defined but never recorded. This plan adds the missing instrumentation calls.

Output: Fully instrumented metrics - latency histogram records check duration, active counters gauge reflects current state.
</objective>

<execution_context>
@/Users/jrtipton/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jrtipton/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-rate-limiter-service-foundation/17-VERIFICATION.md

# Files to modify
@packages/ratelimiter-service/src/ratelimiter_service/metrics.py
@packages/ratelimiter-service/src/ratelimiter_service/api/rate_limit.py
@packages/ratelimiter-service/src/ratelimiter_service/api/management.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add latency recording to rate limit check endpoint</name>
  <files>packages/ratelimiter-service/src/ratelimiter_service/api/rate_limit.py</files>
  <action>
Add CHECK_LATENCY.time() decorator to the check_rate_limit endpoint to record request latency.

1. Import CHECK_LATENCY from metrics.py (add to existing import line 10)
2. Add the @CHECK_LATENCY.time() decorator to check_rate_limit function (before the existing decorators)

The decorator approach is cleaner than manual .observe() calls - it automatically times the entire function execution.

Existing code pattern (line 10):
```python
from ..metrics import record_rate_limit_check
```

Change to:
```python
from ..metrics import record_rate_limit_check, CHECK_LATENCY
```

And add decorator before the endpoint (before line 37):
```python
@CHECK_LATENCY.time()
@rate_limit_router.post("/check", response_model=RateLimitResponse)
async def check_rate_limit(...):
```
  </action>
  <verify>
Run: `cd /Users/jrtipton/x/operator && grep -n "CHECK_LATENCY" packages/ratelimiter-service/src/ratelimiter_service/api/rate_limit.py`

Expected: Shows import line and decorator usage
  </verify>
  <done>CHECK_LATENCY histogram records latency for every rate limit check</done>
</task>

<task type="auto">
  <name>Task 2: Add active counters gauge update to management endpoint</name>
  <files>packages/ratelimiter-service/src/ratelimiter_service/api/management.py</files>
  <action>
Add set_active_counters call in get_counters endpoint after building the counters list.

1. Import set_active_counters from metrics.py (add new import around line 11)
2. Call set_active_counters(len(counters)) after building the counters list, before returning (around line 106)

Add import:
```python
from ..metrics import set_active_counters
```

Add call before return statement in get_counters (after the for loop, before return):
```python
    # Update active counters metric
    set_active_counters(len(counters))

    return CountersResponse(counters=counters)
```
  </action>
  <verify>
Run: `cd /Users/jrtipton/x/operator && grep -n "set_active_counters" packages/ratelimiter-service/src/ratelimiter_service/api/management.py`

Expected: Shows import line and call in get_counters function
  </verify>
  <done>ACTIVE_COUNTERS gauge updated with current count every time /api/counters is called</done>
</task>

</tasks>

<verification>
After both tasks:

1. Verify CHECK_LATENCY wiring:
```bash
cd /Users/jrtipton/x/operator
grep -n "CHECK_LATENCY" packages/ratelimiter-service/src/ratelimiter_service/api/rate_limit.py
```
Expected: Import and @CHECK_LATENCY.time() decorator present

2. Verify set_active_counters wiring:
```bash
grep -n "set_active_counters" packages/ratelimiter-service/src/ratelimiter_service/api/management.py
```
Expected: Import and function call present

3. Run existing tests to ensure no regressions:
```bash
cd /Users/jrtipton/x/operator/packages/ratelimiter-service
uv run pytest tests/ -v
```
Expected: All tests pass

4. Optional - verify metrics are exported (requires running service):
```bash
# Start service: RATELIMITER_REDIS_URL=redis://localhost:6379 uv run uvicorn ratelimiter_service.main:app
# Check metrics: curl localhost:8000/metrics | grep -E "ratelimiter_check_duration|ratelimiter_active_counters"
```
</verification>

<success_criteria>
1. CHECK_LATENCY.time() decorator applied to check_rate_limit endpoint
2. set_active_counters(len(counters)) called in get_counters endpoint
3. All existing tests pass
4. No new imports beyond the metrics module functions
</success_criteria>

<output>
After completion, create `.planning/phases/17-rate-limiter-service-foundation/17-04-SUMMARY.md`
</output>
