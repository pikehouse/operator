---
phase: 36-analysis-layer
plan: 04
type: execute
wave: 3
depends_on: ["36-01", "36-02", "36-03"]
files_modified:
  - eval/src/eval/cli.py
  - eval/src/eval/analysis/__init__.py
autonomous: true

must_haves:
  truths:
    - "Developer can run 'eval analyze <campaign_id>' and see scores in plain text"
    - "Developer can run 'eval compare <campaign_a> <campaign_b>' and see comparison"
    - "Developer can run 'eval compare-baseline <campaign_id>' and see agent vs baseline"
    - "--json flag outputs machine-readable JSON format"
  artifacts:
    - path: "eval/src/eval/cli.py"
      provides: "analyze, compare, compare-baseline CLI commands"
      exports: ["app"]
  key_links:
    - from: "eval/src/eval/cli.py"
      to: "eval/src/eval/analysis/scoring.py"
      via: "analyze_campaign for campaign analysis"
      pattern: "from eval.analysis import analyze_campaign"
    - from: "eval/src/eval/cli.py"
      to: "eval/src/eval/analysis/comparison.py"
      via: "compare_baseline, compare_campaigns for comparisons"
      pattern: "from eval.analysis import compare_baseline, compare_campaigns"
---

<objective>
Add CLI commands for analysis and comparison

Purpose: Implement CLI-04 (analyze), CLI-05 (compare), CLI-06 (compare-baseline) using plain text output by default with --json flag for machine-readable format.

Output: Updated `eval/src/eval/cli.py` with analyze, compare, compare-baseline commands
</objective>

<execution_context>
@/Users/jrtipton/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jrtipton/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/36-analysis-layer/36-RESEARCH.md
@.planning/phases/36-analysis-layer/36-CONTEXT.md
@eval/src/eval/cli.py
@eval/src/eval/analysis/types.py
@eval/src/eval/analysis/scoring.py
@eval/src/eval/analysis/comparison.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update analysis module exports</name>
  <files>eval/src/eval/analysis/__init__.py</files>
  <action>
Ensure `__init__.py` exports all analysis types and functions needed by CLI:

```python
"""Analysis module for evaluation harness.

Provides scoring, command analysis, and comparison functions.
"""

# Types
from eval.analysis.types import (
    TrialOutcome,
    TrialScore,
    CampaignSummary,
)

# Scoring
from eval.analysis.scoring import (
    score_trial,
    analyze_campaign,
)

# Commands
from eval.analysis.commands import (
    CommandCategory,
    CommandClassification,
    CommandAnalysis,
    analyze_commands,
    detect_thrashing,
)

# Comparison
from eval.analysis.comparison import (
    BaselineComparison,
    CampaignComparison,
    compare_baseline,
    compare_campaigns,
)

__all__ = [
    # Types
    "TrialOutcome",
    "TrialScore",
    "CampaignSummary",
    # Scoring
    "score_trial",
    "analyze_campaign",
    # Commands
    "CommandCategory",
    "CommandClassification",
    "CommandAnalysis",
    "analyze_commands",
    "detect_thrashing",
    # Comparison
    "BaselineComparison",
    "CampaignComparison",
    "compare_baseline",
    "compare_campaigns",
]
```
  </action>
  <verify>
```bash
cd /Users/jrtipton/x/operator/eval
python -c "
from eval.analysis import (
    analyze_campaign, compare_baseline, compare_campaigns,
    CampaignSummary, BaselineComparison, CampaignComparison
)
print('All exports OK')
"
```
  </verify>
  <done>Analysis module exports all types and functions needed by CLI</done>
</task>

<task type="auto">
  <name>Task 2: Add analyze command</name>
  <files>eval/src/eval/cli.py</files>
  <action>
Add `eval analyze <campaign_id>` command (CLI-04).

Key implementation details from CONTEXT.md:
- Plain text output by default (no colors)
- --json flag for machine-readable output
- Moderate verbosity: summary + key metrics breakdown

Add to cli.py after the run_app commands:

```python
@app.command()
def analyze(
    campaign_id: int = typer.Argument(..., help="Campaign ID to analyze"),
    db_path: Path = typer.Option(
        Path("eval.db"),
        "--db",
        help="Path to eval database",
    ),
    json_output: bool = typer.Option(
        False,
        "--json",
        help="Output as JSON",
    ),
) -> None:
    """Analyze a campaign and display scores.

    Computes win rate, average detection/resolution times, and outcome breakdown.

    Examples:
        eval analyze 1
        eval analyze 1 --json
    """
    from eval.analysis import analyze_campaign, CampaignSummary

    async def run():
        db = EvalDB(db_path)
        await db.ensure_schema()
        return await analyze_campaign(db, campaign_id)

    summary: CampaignSummary = asyncio.run(run())

    if json_output:
        print(summary.model_dump_json(indent=2))
        return

    # Plain text output
    print(f"Campaign {campaign_id}: {summary.subject_name}/{summary.chaos_type}")
    print(f"Trials: {summary.trial_count}")
    print()
    print("Outcomes:")
    print(f"  Success: {summary.success_count} ({summary.win_rate:.1%})")
    print(f"  Failure: {summary.failure_count}")
    print(f"  Timeout: {summary.timeout_count}")
    print()
    print("Timing (successful trials):")
    if summary.avg_time_to_detect_sec is not None:
        print(f"  Avg detection: {summary.avg_time_to_detect_sec:.1f}s")
    else:
        print("  Avg detection: N/A")
    if summary.avg_time_to_resolve_sec is not None:
        print(f"  Avg resolution: {summary.avg_time_to_resolve_sec:.1f}s")
    else:
        print("  Avg resolution: N/A")
```
  </action>
  <verify>
```bash
cd /Users/jrtipton/x/operator/eval
# Check command is registered
python -c "from eval.cli import app; print([c.name for c in app.registered_commands])"
# Should include 'analyze'
```
  </verify>
  <done>eval analyze <campaign_id> command added with plain text and --json output</done>
</task>

<task type="auto">
  <name>Task 3: Add compare and compare-baseline commands</name>
  <files>eval/src/eval/cli.py</files>
  <action>
Add `eval compare` (CLI-05) and `eval compare-baseline` (CLI-06) commands.

```python
@app.command()
def compare(
    campaign_a: int = typer.Argument(..., help="First campaign ID"),
    campaign_b: int = typer.Argument(..., help="Second campaign ID"),
    db_path: Path = typer.Option(
        Path("eval.db"),
        "--db",
        help="Path to eval database",
    ),
    json_output: bool = typer.Option(
        False,
        "--json",
        help="Output as JSON",
    ),
) -> None:
    """Compare two campaigns by win rate.

    Primary metric is win rate. Resolution time is tiebreaker for equal win rates.

    Examples:
        eval compare 1 2
        eval compare 1 2 --json
    """
    from eval.analysis import compare_campaigns, CampaignComparison

    async def run():
        db = EvalDB(db_path)
        await db.ensure_schema()
        return await compare_campaigns(db, campaign_a, campaign_b)

    try:
        result: CampaignComparison = asyncio.run(run())
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)

    if json_output:
        print(result.model_dump_json(indent=2))
        return

    # Plain text output
    print(f"Campaign Comparison: {result.subject_name}/{result.chaos_type}")
    print()
    print(f"{'Metric':<20} {'Campaign A':<15} {'Campaign B':<15} {'Delta':<15}")
    print("-" * 65)
    print(f"{'Trials':<20} {result.a_trial_count:<15} {result.b_trial_count:<15} {'':<15}")
    print(f"{'Win Rate':<20} {result.a_win_rate:.1%:<15} {result.b_win_rate:.1%:<15} {result.win_rate_delta:+.1%}")

    a_resolve = f"{result.a_avg_resolve_sec:.1f}s" if result.a_avg_resolve_sec else "N/A"
    b_resolve = f"{result.b_avg_resolve_sec:.1f}s" if result.b_avg_resolve_sec else "N/A"
    delta_resolve = f"{result.resolve_time_delta:+.1f}s" if result.resolve_time_delta else ""
    print(f"{'Avg Resolution':<20} {a_resolve:<15} {b_resolve:<15} {delta_resolve}")
    print()
    print(f"Winner: Campaign {result.winner}")
    print(f"Reason: {result.winner_reason}")


@app.command("compare-baseline")
def compare_baseline_cmd(
    campaign_id: int = typer.Argument(..., help="Agent campaign ID"),
    baseline_id: Optional[int] = typer.Option(
        None,
        "--baseline",
        "-b",
        help="Baseline campaign ID (auto-detects if not specified)",
    ),
    db_path: Path = typer.Option(
        Path("eval.db"),
        "--db",
        help="Path to eval database",
    ),
    json_output: bool = typer.Option(
        False,
        "--json",
        help="Output as JSON",
    ),
) -> None:
    """Compare agent campaign to baseline (self-healing).

    Shows full metric breakdown: win rate, detection time, resolution time.
    Auto-detects matching baseline campaign if not specified.

    Examples:
        eval compare-baseline 1
        eval compare-baseline 1 --baseline 2
        eval compare-baseline 1 --json
    """
    from eval.analysis import compare_baseline, BaselineComparison

    async def run():
        db = EvalDB(db_path)
        await db.ensure_schema()
        return await compare_baseline(db, campaign_id, baseline_id)

    try:
        result: BaselineComparison = asyncio.run(run())
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)

    if json_output:
        print(result.model_dump_json(indent=2))
        return

    # Plain text output
    print(f"Baseline Comparison: {result.subject_name}/{result.chaos_type}")
    print(f"Agent Campaign: {result.agent_campaign_id}")
    print(f"Baseline Campaign: {result.baseline_campaign_id}")
    print()
    print(f"{'Metric':<20} {'Agent':<15} {'Baseline':<15} {'Delta':<15}")
    print("-" * 65)
    print(f"{'Trials':<20} {result.agent_trial_count:<15} {result.baseline_trial_count:<15} {'':<15}")
    print(f"{'Win Rate':<20} {result.agent_win_rate:.1%:<15} {result.baseline_win_rate:.1%:<15} {result.win_rate_delta:+.1%}")

    agent_detect = f"{result.agent_avg_detect_sec:.1f}s" if result.agent_avg_detect_sec else "N/A"
    print(f"{'Avg Detection':<20} {agent_detect:<15} {'N/A':<15} {'':<15}")

    agent_resolve = f"{result.agent_avg_resolve_sec:.1f}s" if result.agent_avg_resolve_sec else "N/A"
    baseline_resolve = f"{result.baseline_avg_resolve_sec:.1f}s" if result.baseline_avg_resolve_sec else "N/A"
    delta_resolve = f"{result.resolve_time_delta:+.1f}s" if result.resolve_time_delta else ""
    print(f"{'Avg Resolution':<20} {agent_resolve:<15} {baseline_resolve:<15} {delta_resolve}")
    print()
    print(f"Winner: {result.winner.title()}")
    print(f"Reason: {result.winner_reason}")
```

Also add the missing import at the top of cli.py if not present:
```python
from typing import Optional
```
  </action>
  <verify>
```bash
cd /Users/jrtipton/x/operator/eval
# Check commands are registered
python -c "
from eval.cli import app
commands = [c.name for c in app.registered_commands]
print(f'Commands: {commands}')
assert 'analyze' in commands, 'Missing analyze'
assert 'compare' in commands, 'Missing compare'
assert 'compare-baseline' in commands, 'Missing compare-baseline'
print('All CLI commands registered')
"
```
  </verify>
  <done>eval compare and eval compare-baseline commands added with plain text and --json output</done>
</task>

</tasks>

<verification>
After all tasks:
1. `eval analyze 1` shows campaign summary in plain text
2. `eval analyze 1 --json` outputs JSON
3. `eval compare 1 2` shows side-by-side comparison
4. `eval compare-baseline 1` shows agent vs baseline
5. All commands have --json flag for machine-readable output
6. Requirements CLI-04, CLI-05, CLI-06 satisfied
</verification>

<success_criteria>
- eval analyze <campaign_id> displays campaign summary
- eval compare <a> <b> displays win rate comparison with winner
- eval compare-baseline <id> displays agent vs baseline breakdown
- --json flag outputs Pydantic model JSON
- Plain text output has no colors (pipeable)
- Error messages displayed for mismatched campaigns
</success_criteria>

<output>
After completion, create `.planning/phases/36-analysis-layer/36-04-SUMMARY.md`
</output>
