---
phase: 36-analysis-layer
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - eval/src/eval/analysis/__init__.py
  - eval/src/eval/analysis/types.py
  - eval/src/eval/analysis/scoring.py
autonomous: true

must_haves:
  truths:
    - "TrialScore dataclass captures resolution status, detection time, and resolution time"
    - "score_trial() computes scores from raw Trial data without mutating database"
    - "CampaignSummary aggregates win rate and average times across trials"
    - "Analysis functions are idempotent (same input produces same output)"
  artifacts:
    - path: "eval/src/eval/analysis/types.py"
      provides: "TrialScore, CampaignSummary, TrialOutcome dataclasses"
      exports: ["TrialScore", "CampaignSummary", "TrialOutcome"]
    - path: "eval/src/eval/analysis/scoring.py"
      provides: "Trial scoring and campaign analysis functions"
      exports: ["score_trial", "analyze_campaign"]
  key_links:
    - from: "eval/src/eval/analysis/scoring.py"
      to: "eval/src/eval/runner/db.py"
      via: "EvalDB for reading trial data"
      pattern: "from eval.runner.db import EvalDB"
    - from: "eval/src/eval/analysis/scoring.py"
      to: "eval/src/eval/types.py"
      via: "Trial dataclass for input"
      pattern: "from eval.types import Trial"
---

<objective>
Create analysis types and scoring module for computing trial metrics

Purpose: Establish foundation types (TrialScore, CampaignSummary) and implement time-to-detect/time-to-resolve calculations from raw trial data. This enables ANAL-01 (scoring), ANAL-06 (idempotent analysis).

Output: `eval/src/eval/analysis/` package with types.py and scoring.py
</objective>

<execution_context>
@/Users/jrtipton/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jrtipton/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/36-analysis-layer/36-RESEARCH.md
@eval/src/eval/types.py
@eval/src/eval/runner/db.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create analysis types module</name>
  <files>eval/src/eval/analysis/__init__.py, eval/src/eval/analysis/types.py</files>
  <action>
Create `eval/src/eval/analysis/` directory with `__init__.py` and `types.py`.

In `types.py`, define these dataclasses using Pydantic BaseModel for JSON serialization:

```python
from enum import Enum
from pydantic import BaseModel

class TrialOutcome(str, Enum):
    SUCCESS = "success"    # Resolved and healthy
    FAILURE = "failure"    # Not resolved or not healthy
    TIMEOUT = "timeout"    # Timed out waiting

class TrialScore(BaseModel):
    """Computed metrics for a single trial."""
    trial_id: int
    outcome: TrialOutcome
    resolved: bool  # Ticket resolved AND final state healthy
    time_to_detect_sec: float | None  # chaos_injected -> ticket_created
    time_to_resolve_sec: float | None  # chaos_injected -> resolved
    command_count: int = 0
    unique_commands: int = 0
    destructive_count: int = 0

class CampaignSummary(BaseModel):
    """Aggregate metrics for a campaign."""
    campaign_id: int
    subject_name: str
    chaos_type: str
    trial_count: int
    success_count: int
    failure_count: int
    timeout_count: int
    win_rate: float  # success_count / trial_count
    avg_time_to_detect_sec: float | None
    avg_time_to_resolve_sec: float | None
```

In `__init__.py`, re-export the types:
```python
from eval.analysis.types import TrialOutcome, TrialScore, CampaignSummary
```
  </action>
  <verify>
```bash
cd /Users/jrtipton/x/operator/eval
python -c "from eval.analysis import TrialScore, CampaignSummary, TrialOutcome; print('Types OK')"
```
  </verify>
  <done>TrialScore, CampaignSummary, TrialOutcome dataclasses importable from eval.analysis</done>
</task>

<task type="auto">
  <name>Task 2: Implement scoring module</name>
  <files>eval/src/eval/analysis/scoring.py</files>
  <action>
Create `scoring.py` with idempotent analysis functions.

Key implementation details from RESEARCH.md:
- Use `datetime.fromisoformat()` for ISO8601 parsing (preserves timezone)
- Resolution requires BOTH: `resolved_at is not None` AND `final_state` indicates healthy
- For health check: parse `final_state` JSON, check all nodes running (for TiKV: store_count == node_count)
- Time calculations in seconds using `timedelta.total_seconds()`
- Functions are pure: read from Trial/db, return dataclass, no mutations

```python
import json
from datetime import datetime
from statistics import mean

from eval.types import Trial
from eval.runner.db import EvalDB
from eval.analysis.types import TrialScore, CampaignSummary, TrialOutcome


def compute_duration_seconds(start_iso: str, end_iso: str | None) -> float | None:
    """Compute duration in seconds between ISO8601 timestamps."""
    if end_iso is None:
        return None
    start = datetime.fromisoformat(start_iso)
    end = datetime.fromisoformat(end_iso)
    return (end - start).total_seconds()


def is_final_state_healthy(final_state_json: str, subject_name: str) -> bool:
    """Determine if final state represents healthy cluster.

    Subject-specific health checks:
    - tikv: all stores in 'Up' state
    - Other subjects: default to True if final_state exists
    """
    try:
        state = json.loads(final_state_json)
    except json.JSONDecodeError:
        return False

    if subject_name.lower() == "tikv":
        # TiKV health: check stores exist and are up
        stores = state.get("stores", [])
        if not stores:
            return False
        return all(s.get("state_name") == "Up" for s in stores)

    # Default: if we have state, assume healthy (baseline may not have ticket)
    return bool(state)


def score_trial(trial: Trial, subject_name: str) -> TrialScore:
    """Compute trial score from stored data (idempotent).

    ANAL-01: Computes time-to-detect, time-to-resolve
    ANAL-06: Idempotent - no database mutations
    """
    # Time-to-detect: chaos_injected -> ticket_created
    time_to_detect = compute_duration_seconds(
        trial.chaos_injected_at,
        trial.ticket_created_at
    )

    # Time-to-resolve: chaos_injected -> resolved
    time_to_resolve = compute_duration_seconds(
        trial.chaos_injected_at,
        trial.resolved_at
    )

    # Resolution: ticket resolved AND cluster healthy
    final_healthy = is_final_state_healthy(trial.final_state, subject_name)
    resolved = trial.resolved_at is not None and final_healthy

    # Determine outcome
    if resolved:
        outcome = TrialOutcome.SUCCESS
    elif trial.resolved_at is None and not final_healthy:
        outcome = TrialOutcome.TIMEOUT
    else:
        outcome = TrialOutcome.FAILURE

    # Command counts (populated by commands.py later)
    commands = json.loads(trial.commands_json) if trial.commands_json else []

    return TrialScore(
        trial_id=trial.id or 0,
        outcome=outcome,
        resolved=resolved,
        time_to_detect_sec=time_to_detect,
        time_to_resolve_sec=time_to_resolve,
        command_count=len(commands),
        unique_commands=len(set(c.get("tool_params", "") for c in commands)),
        destructive_count=0,  # Set by commands.py
    )


async def analyze_campaign(db: EvalDB, campaign_id: int) -> CampaignSummary:
    """Compute campaign summary (idempotent, no database mutations).

    ANAL-01: Aggregates time-to-detect, time-to-resolve
    ANAL-06: Idempotent - reads only
    """
    campaign = await db.get_campaign(campaign_id)
    if not campaign:
        raise ValueError(f"Campaign {campaign_id} not found")

    trials = await db.get_trials(campaign_id)

    # Score each trial
    scores = [score_trial(t, campaign.subject_name) for t in trials]

    # Count outcomes
    success_count = sum(1 for s in scores if s.outcome == TrialOutcome.SUCCESS)
    failure_count = sum(1 for s in scores if s.outcome == TrialOutcome.FAILURE)
    timeout_count = sum(1 for s in scores if s.outcome == TrialOutcome.TIMEOUT)

    win_rate = success_count / len(scores) if scores else 0.0

    # Average times (only for successful trials)
    detect_times = [s.time_to_detect_sec for s in scores if s.time_to_detect_sec is not None]
    resolve_times = [s.time_to_resolve_sec for s in scores if s.time_to_resolve_sec is not None]

    return CampaignSummary(
        campaign_id=campaign_id,
        subject_name=campaign.subject_name,
        chaos_type=campaign.chaos_type,
        trial_count=len(trials),
        success_count=success_count,
        failure_count=failure_count,
        timeout_count=timeout_count,
        win_rate=win_rate,
        avg_time_to_detect_sec=mean(detect_times) if detect_times else None,
        avg_time_to_resolve_sec=mean(resolve_times) if resolve_times else None,
    )
```

Update `__init__.py` to re-export scoring functions:
```python
from eval.analysis.scoring import score_trial, analyze_campaign
```
  </action>
  <verify>
```bash
cd /Users/jrtipton/x/operator/eval
python -c "
from eval.analysis import score_trial, analyze_campaign, TrialScore
from eval.types import Trial
# Test with mock trial
t = Trial(
    id=1, campaign_id=1,
    started_at='2026-01-29T10:00:00+00:00',
    chaos_injected_at='2026-01-29T10:01:00+00:00',
    ticket_created_at='2026-01-29T10:01:30+00:00',
    resolved_at='2026-01-29T10:02:00+00:00',
    ended_at='2026-01-29T10:03:00+00:00',
    initial_state='{}',
    final_state='{\"stores\": [{\"state_name\": \"Up\"}]}',
    chaos_metadata='{}',
    commands_json='[]'
)
score = score_trial(t, 'tikv')
print(f'Score: resolved={score.resolved}, detect={score.time_to_detect_sec}s, resolve={score.time_to_resolve_sec}s')
assert score.resolved == True
assert score.time_to_detect_sec == 30.0
assert score.time_to_resolve_sec == 60.0
print('Scoring OK')
"
```
  </verify>
  <done>score_trial() computes time-to-detect (30s) and time-to-resolve (60s) correctly from Trial data</done>
</task>

</tasks>

<verification>
After both tasks:
1. `from eval.analysis import TrialScore, CampaignSummary, TrialOutcome` works
2. `from eval.analysis import score_trial, analyze_campaign` works
3. score_trial() computes correct times from ISO8601 timestamps
4. Analysis functions don't mutate database (idempotent per ANAL-06)
</verification>

<success_criteria>
- Types module provides TrialScore, CampaignSummary, TrialOutcome
- Scoring module computes time-to-detect and time-to-resolve from Trial data
- Health check uses subject-specific logic (TiKV stores in 'Up' state)
- All functions are pure/idempotent (read only, return dataclass)
- Requirements ANAL-01 and ANAL-06 satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/36-analysis-layer/36-01-SUMMARY.md`
</output>
