---
phase: 31-agent-loop
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/operator-core/src/operator_core/agent_lab/loop.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Tool calls logged to audit database when Claude requests tool use"
    - "Tool results logged to audit database after tool execution"
    - "Audit entries include tool_name, tool_params, and exit_code"
    - "Loop stays under 200 lines"
  artifacts:
    - path: "packages/operator-core/src/operator_core/agent_lab/loop.py"
      provides: "Complete audit logging for tool calls and results"
      contains: "ToolUseBlock"
  key_links:
    - from: "loop.py"
      to: "audit_db.log_entry"
      via: "tool_call entry type"
      pattern: 'log_entry.*"tool_call"'
    - from: "loop.py"
      to: "audit_db.log_entry"
      via: "tool_result entry type"
      pattern: 'log_entry.*"tool_result"'
---

<objective>
Close the verification gap: Add tool call and tool result logging to the agent loop.

Purpose: The audit trail is incomplete - it only logs reasoning text, not the commands Claude runs or their outputs. This prevents replay/review of sessions.

Output: Updated loop.py that logs tool_call entries when Claude requests shell execution and tool_result entries after execution completes.
</objective>

<execution_context>
@/Users/jrtipton/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jrtipton/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/31-agent-loop/31-agent-loop-VERIFICATION.md

# Source file to modify
@packages/operator-core/src/operator_core/agent_lab/loop.py

# Audit log interface (already supports tool_call/tool_result)
@packages/operator-core/src/operator_core/db/audit_log.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add tool call and tool result logging to process_ticket</name>
  <files>packages/operator-core/src/operator_core/agent_lab/loop.py</files>
  <action>
Modify the for loop in process_ticket() that iterates through message.content blocks. Currently it only handles text blocks. Add handling for:

1. **ToolUseBlock detection:** Import ToolUseBlock from anthropic.types (or check for block.type == "tool_use"). When detected:
   - Extract tool_name from block.name
   - Extract tool_params from block.input (this is a dict with command and reasoning)
   - Log to audit_db with entry_type="tool_call", tool_name, tool_params
   - Print summary: `[Tool Call] shell: {command}`

2. **ToolResultBlock detection:** Import ToolResultBlock or check for block.type == "tool_result". When detected:
   - Extract content (the tool output string)
   - Summarize with Haiku before logging
   - Parse exit_code from the content if present (look for "Exit code: N" pattern at end)
   - Log to audit_db with entry_type="tool_result", content=summary, raw_content=full_output, exit_code
   - Print summary: `[Tool Result] {summary}`

The updated loop structure should be approximately:

```python
for message in runner:
    if message.content:
        for block in message.content:
            if hasattr(block, "text") and block.text:
                # Existing reasoning handling
                summary = summarize_with_haiku(client, block.text)
                audit_db.log_entry(session_id, "reasoning", summary, block.text, None, None, None)
                print(f"[Claude] {summary}")
            elif block.type == "tool_use":
                # NEW: Log tool call
                tool_params = block.input
                audit_db.log_entry(session_id, "tool_call", f"shell: {tool_params.get('command', '')[:50]}", None, block.name, tool_params, None)
                print(f"[Tool Call] shell: {tool_params.get('command', '')}")
            elif block.type == "tool_result":
                # NEW: Log tool result
                content = block.content if isinstance(block.content, str) else str(block.content)
                summary = summarize_with_haiku(client, content)
                exit_code = parse_exit_code(content)  # Helper function or inline
                audit_db.log_entry(session_id, "tool_result", summary, content, "shell", None, exit_code)
                print(f"[Tool Result] {summary}")
    final_message = message
```

**Important constraints:**
- Keep total file under 200 lines (currently 171, budget ~29 lines)
- Do NOT import types at module level if they add overhead - check block.type attribute instead
- Exit code parsing can be inline (no separate function needed if under 200 lines is tight)
- The tool_runner messages include both assistant and user role messages as it runs
  </action>
  <verify>
1. Run type check: `cd /Users/jrtipton/x/operator && uv run --package operator-core pyright packages/operator-core/src/operator_core/agent_lab/loop.py`
2. Count lines: `wc -l packages/operator-core/src/operator_core/agent_lab/loop.py` (must be < 200)
3. Grep for tool_call logging: `grep -n "tool_call" packages/operator-core/src/operator_core/agent_lab/loop.py`
4. Grep for tool_result logging: `grep -n "tool_result" packages/operator-core/src/operator_core/agent_lab/loop.py`
  </verify>
  <done>
- Loop handles ToolUseBlock and logs entry_type="tool_call" with tool_name and tool_params
- Loop handles ToolResultBlock and logs entry_type="tool_result" with content, raw_content, and exit_code
- File remains under 200 lines
- Type checking passes
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify complete audit trail with unit test</name>
  <files>packages/operator-core/tests/test_loop_audit.py</files>
  <action>
Create a focused test that verifies the audit log entries are created correctly. Since running the actual agent loop requires an Anthropic API key and running Claude, create a unit test that:

1. Mocks the anthropic client and tool_runner to produce predictable messages
2. Calls process_ticket() with the mock
3. Verifies audit_db.log_entry was called with:
   - At least one "reasoning" entry
   - At least one "tool_call" entry with tool_name="shell"
   - At least one "tool_result" entry with exit_code present

Use pytest and unittest.mock. The test should:
- Create a temporary SQLite database
- Set up mock messages that simulate: text block -> tool_use block -> tool_result block
- Call process_ticket
- Query the database to verify all three entry types exist

Keep test under 60 lines.
  </action>
  <verify>
1. Run the test: `cd /Users/jrtipton/x/operator && uv run --package operator-core pytest packages/operator-core/tests/test_loop_audit.py -v`
2. Test should pass with assertions for reasoning, tool_call, and tool_result entries
  </verify>
  <done>
- Test file exists at packages/operator-core/tests/test_loop_audit.py
- Test mocks tool_runner messages and verifies all three entry types logged
- Test passes
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. **Line count check:**
   ```bash
   wc -l packages/operator-core/src/operator_core/agent_lab/loop.py
   ```
   Must be < 200 lines

2. **Type check:**
   ```bash
   cd /Users/jrtipton/x/operator && uv run --package operator-core pyright packages/operator-core/src/operator_core/agent_lab/loop.py
   ```
   No errors

3. **Unit test:**
   ```bash
   cd /Users/jrtipton/x/operator && uv run --package operator-core pytest packages/operator-core/tests/test_loop_audit.py -v
   ```
   All tests pass

4. **Code inspection:**
   - Grep confirms "tool_call" and "tool_result" entry types present in loop.py
   - Audit entries include tool_name, tool_params, exit_code as appropriate
</verification>

<success_criteria>
- Tool calls logged to audit database when Claude requests tool use
- Tool results logged to audit database after tool execution
- Exit codes captured from tool results
- Loop.py remains under 200 lines
- Type checking passes
- Unit test verifies complete audit trail
</success_criteria>

<output>
After completion, create `.planning/phases/31-agent-loop/31-02-SUMMARY.md`
</output>
