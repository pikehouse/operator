---
phase: 39-config-variants
plan: 03
type: execute
wave: 3
depends_on: ["39-02"]
files_modified:
  - eval/src/eval/analysis/comparison.py
  - eval/src/eval/analysis/__init__.py
  - eval/src/eval/cli.py
autonomous: true

must_haves:
  truths:
    - "Developer can run compare-variants CLI command"
    - "Comparison shows aggregate metrics per variant"
    - "Output displays balanced scorecard table"
  artifacts:
    - path: "eval/src/eval/analysis/comparison.py"
      provides: "compare_variants function and VariantComparison model"
      contains: "class VariantComparison"
      exports: ["compare_variants", "VariantComparison", "VariantMetrics"]
    - path: "eval/src/eval/cli.py"
      provides: "compare-variants CLI command"
      contains: "def compare_variants_cmd"
  key_links:
    - from: "eval/src/eval/cli.py"
      to: "eval/src/eval/analysis/comparison.py"
      via: "imports compare_variants"
      pattern: "from eval.analysis import.*compare_variants"
---

<objective>
Create variant comparison analysis command with balanced scorecard output.

Purpose: Enable developers to compare agent performance across different variants (model, prompt, tools).
Output: compare_variants function, VariantComparison model, compare-variants CLI command with Rich table.
</objective>

<execution_context>
@/Users/jrtipton/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jrtipton/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/39-config-variants/39-CONTEXT.md
@.planning/phases/39-config-variants/39-RESEARCH.md
@.planning/phases/39-config-variants/39-02-SUMMARY.md
@eval/src/eval/analysis/comparison.py
@eval/src/eval/analysis/__init__.py
@eval/src/eval/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add VariantMetrics and VariantComparison models</name>
  <files>eval/src/eval/analysis/comparison.py</files>
  <action>
Add two new Pydantic models at the end of comparison.py (after CampaignComparison class):

```python
class VariantMetrics(BaseModel):
    """Aggregate metrics for a single variant across all its campaigns/trials."""
    variant_name: str
    trial_count: int
    success_count: int
    win_rate: float
    avg_time_to_detect_sec: float | None
    avg_time_to_resolve_sec: float | None
    avg_commands: float


class VariantComparison(BaseModel):
    """Comparison of multiple variants for the same subject/chaos combination.

    Shows balanced scorecard - all metrics equally, no winner determination.
    User interprets tradeoffs.
    """
    subject_name: str
    chaos_type: str
    variants: dict[str, VariantMetrics]  # variant_name -> metrics
```
  </action>
  <verify>
Run: `cd /Users/jrtipton/x/operator/eval && python -c "
from eval.analysis.comparison import VariantMetrics, VariantComparison
m = VariantMetrics(variant_name='test', trial_count=5, success_count=4, win_rate=0.8, avg_time_to_detect_sec=10.0, avg_time_to_resolve_sec=30.0, avg_commands=15.0)
c = VariantComparison(subject_name='tikv', chaos_type='node_kill', variants={'test': m})
print(f'VariantComparison has {len(c.variants)} variant(s)')
"`
  </verify>
  <done>VariantMetrics and VariantComparison models validate correctly</done>
</task>

<task type="auto">
  <name>Task 2: Implement compare_variants function</name>
  <files>eval/src/eval/analysis/comparison.py, eval/src/eval/analysis/__init__.py</files>
  <action>
1. Add compare_variants function to comparison.py after the existing functions:

```python
async def compare_variants(
    db: EvalDB,
    subject_name: str,
    chaos_type: str,
    variant_names: list[str] | None = None,
) -> VariantComparison:
    """Compare performance across variants for same subject/chaos combination.

    CONF-03: Aggregate metrics per variant for A/B comparison.

    Args:
        db: EvalDB instance
        subject_name: Filter by subject (e.g., "tikv")
        chaos_type: Filter by chaos type (e.g., "node_kill")
        variant_names: Optional list of variant names to include (None = all)

    Returns:
        VariantComparison with aggregate metrics per variant

    Raises:
        ValueError: If no campaigns found for criteria
    """
    import aiosqlite

    async with aiosqlite.connect(db.db_path) as conn:
        conn.row_factory = aiosqlite.Row

        # Check if variant_name column exists (for backward compatibility)
        cursor = await conn.execute("PRAGMA table_info(campaigns)")
        columns = await cursor.fetchall()
        has_variant_column = any(col[1] == "variant_name" for col in columns)

        if not has_variant_column:
            raise ValueError("Database schema missing variant_name column. Run migration first.")

        # Get all non-baseline campaigns matching criteria
        query = """
            SELECT id, variant_name
            FROM campaigns
            WHERE subject_name = ? AND chaos_type = ? AND baseline = 0
        """
        params: list = [subject_name, chaos_type]

        if variant_names:
            placeholders = ",".join("?" * len(variant_names))
            query += f" AND variant_name IN ({placeholders})"
            params.extend(variant_names)

        cursor = await conn.execute(query, params)
        campaigns = await cursor.fetchall()

    if not campaigns:
        raise ValueError(
            f"No campaigns found for {subject_name}/{chaos_type}"
            + (f" with variants {variant_names}" if variant_names else "")
        )

    # Group campaigns by variant, analyze each
    variant_summaries: dict[str, list[CampaignSummary]] = {}
    for campaign in campaigns:
        summary = await analyze_campaign(db, campaign["id"])
        variant_name = campaign["variant_name"] or "default"
        if variant_name not in variant_summaries:
            variant_summaries[variant_name] = []
        variant_summaries[variant_name].append(summary)

    # Aggregate metrics for each variant
    def _safe_avg(values: list[float | None]) -> float | None:
        filtered = [v for v in values if v is not None]
        return sum(filtered) / len(filtered) if filtered else None

    results: dict[str, VariantMetrics] = {}
    for variant_name, summaries in variant_summaries.items():
        total_trials = sum(s.trial_count for s in summaries)
        total_success = sum(s.success_count for s in summaries)
        total_commands = sum(s.total_commands for s in summaries)

        results[variant_name] = VariantMetrics(
            variant_name=variant_name,
            trial_count=total_trials,
            success_count=total_success,
            win_rate=total_success / total_trials if total_trials > 0 else 0.0,
            avg_time_to_detect_sec=_safe_avg([s.avg_time_to_detect_sec for s in summaries]),
            avg_time_to_resolve_sec=_safe_avg([s.avg_time_to_resolve_sec for s in summaries]),
            avg_commands=total_commands / total_trials if total_trials > 0 else 0.0,
        )

    return VariantComparison(
        subject_name=subject_name,
        chaos_type=chaos_type,
        variants=results,
    )
```

2. Update `eval/src/eval/analysis/__init__.py` to export the new types:

Add to the imports:
```python
from eval.analysis.comparison import (
    BaselineComparison,
    CampaignComparison,
    VariantMetrics,
    VariantComparison,
    compare_baseline,
    compare_campaigns,
    compare_variants,
)
```

Update __all__ to include the new exports:
```python
__all__ = [
    "analyze_campaign",
    "CampaignSummary",
    "TrialOutcome",
    "compare_baseline",
    "compare_campaigns",
    "compare_variants",
    "BaselineComparison",
    "CampaignComparison",
    "VariantMetrics",
    "VariantComparison",
    "classify_commands",
    "CommandAnalysis",
]
```
  </action>
  <verify>
Run: `cd /Users/jrtipton/x/operator/eval && python -c "
from eval.analysis import compare_variants, VariantComparison, VariantMetrics
print('compare_variants imported successfully')
print(f'VariantComparison fields: {list(VariantComparison.model_fields.keys())}')
"`
  </verify>
  <done>compare_variants function exists and is exported from eval.analysis</done>
</task>

<task type="auto">
  <name>Task 3: Add compare-variants CLI command</name>
  <files>eval/src/eval/cli.py</files>
  <action>
Add the compare-variants command to cli.py. Add after the compare_baseline_cmd function:

```python
@app.command("compare-variants")
def compare_variants_cmd(
    subject: str = typer.Argument(..., help="Subject name (e.g., 'tikv')"),
    chaos: str = typer.Argument(..., help="Chaos type (e.g., 'node_kill')"),
    variants: Optional[str] = typer.Option(
        None,
        "--variants", "-v",
        help="Comma-separated variant names to compare (default: all)",
    ),
    db_path: Path = typer.Option(
        Path("eval.db"),
        "--db",
        help="Path to eval database",
    ),
    json_output: bool = typer.Option(
        False,
        "--json",
        help="Output as JSON",
    ),
) -> None:
    """Compare agent performance across variants.

    Shows balanced scorecard of metrics for each variant tested against
    the same subject and chaos type. No winner determination - user
    interprets tradeoffs.

    Requires campaigns to have been run with different variant configurations.

    Examples:
        eval compare-variants tikv node_kill
        eval compare-variants tikv node_kill --variants haiku-v1,sonnet-v1
        eval compare-variants tikv latency --json
    """
    from eval.analysis import compare_variants, VariantComparison

    # Parse variant names if provided
    variant_list: list[str] | None = None
    if variants:
        variant_list = [v.strip() for v in variants.split(",")]

    async def run():
        db = EvalDB(db_path)
        await db.ensure_schema()
        return await compare_variants(db, subject, chaos, variant_list)

    try:
        result: VariantComparison = asyncio.run(run())
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)

    if json_output:
        print(result.model_dump_json(indent=2))
        return

    # Rich table output - balanced scorecard
    table = Table(title=f"Variant Comparison: {result.subject_name}/{result.chaos_type}")

    table.add_column("Variant", style="cyan")
    table.add_column("Trials", justify="right")
    table.add_column("Success Rate", justify="right")
    table.add_column("Avg TTD", justify="right", header_style="dim")
    table.add_column("Avg TTR", justify="right", header_style="dim")
    table.add_column("Avg Commands", justify="right")

    # Sort by variant name for consistent output
    for variant_name in sorted(result.variants.keys()):
        metrics = result.variants[variant_name]
        ttd = f"{metrics.avg_time_to_detect_sec:.1f}s" if metrics.avg_time_to_detect_sec else "N/A"
        ttr = f"{metrics.avg_time_to_resolve_sec:.1f}s" if metrics.avg_time_to_resolve_sec else "N/A"

        table.add_row(
            variant_name,
            str(metrics.trial_count),
            f"{metrics.win_rate:.1%}",
            ttd,
            ttr,
            f"{metrics.avg_commands:.1f}",
        )

    console.print(table)
    console.print(f"\n[dim]TTD = Time to Detect, TTR = Time to Resolve[/dim]")
    console.print(f"[dim]{len(result.variants)} variant(s) compared[/dim]")
```
  </action>
  <verify>
Run: `cd /Users/jrtipton/x/operator/eval && python -m eval.cli compare-variants --help`

Expected: Help text showing subject, chaos, variants, db, and json options
  </verify>
  <done>compare-variants command shows help with all expected options</done>
</task>

</tasks>

<verification>
1. `cd /Users/jrtipton/x/operator/eval && python -c "from eval.analysis import compare_variants, VariantComparison, VariantMetrics; print('Imports OK')"`
2. `cd /Users/jrtipton/x/operator/eval && python -m eval.cli compare-variants --help` - shows command help
3. Models validate: VariantMetrics with all fields, VariantComparison with variants dict
</verification>

<success_criteria>
- VariantMetrics model with: variant_name, trial_count, success_count, win_rate, avg_time_to_detect_sec, avg_time_to_resolve_sec, avg_commands
- VariantComparison model with: subject_name, chaos_type, variants dict
- compare_variants function queries by subject/chaos, aggregates by variant
- compare-variants CLI command displays Rich table with balanced scorecard
- No winner determination - just data display
</success_criteria>

<output>
After completion, create `.planning/phases/39-config-variants/39-03-SUMMARY.md`
</output>
