---
phase: 34-demo-e2e-validation
plan: 02
type: execute
wave: 2
depends_on: ["34-01"]
files_modified:
  - scripts/integration_tests/__init__.py
  - scripts/integration_tests/test_tikv_demo_e2e.py
  - scripts/integration_tests/test_ratelimiter_demo_e2e.py
  - scripts/integration_tests/conftest.py
autonomous: true

must_haves:
  truths:
    - "pytest can run TiKV demo integration test"
    - "pytest can run ratelimiter demo integration test"
    - "Tests verify monitor detection and agent processing"
    - "Tests clean up subprocesses properly"
  artifacts:
    - path: "scripts/integration_tests/__init__.py"
      provides: "Package marker"
      min_lines: 1
    - path: "scripts/integration_tests/test_tikv_demo_e2e.py"
      provides: "TiKV demo integration test"
      min_lines: 50
    - path: "scripts/integration_tests/test_ratelimiter_demo_e2e.py"
      provides: "Ratelimiter demo integration test"
      min_lines: 50
    - path: "scripts/integration_tests/conftest.py"
      provides: "Shared pytest fixtures"
      min_lines: 20
  key_links:
    - from: "scripts/integration_tests/test_tikv_demo_e2e.py"
      to: "operator_core.tui.subprocess"
      via: "import SubprocessManager"
      pattern: "from operator_core.tui.subprocess import SubprocessManager"
    - from: "scripts/integration_tests/test_ratelimiter_demo_e2e.py"
      to: "operator_core.tui.subprocess"
      via: "import SubprocessManager"
      pattern: "from operator_core.tui.subprocess import SubprocessManager"
---

<objective>
Create pytest-based integration tests that verify both TiKV and rate limiter demos run without errors.

Purpose: Provide automated regression testing for demo flows, enabling CI verification.
Output: Two integration test files that can be run with `uv run pytest scripts/integration_tests/`
</objective>

<execution_context>
@/Users/jrtipton/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jrtipton/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/34-demo-e2e-validation/34-RESEARCH.md
@scripts/test-demo-flow.py
@scripts/test-tui-demo.py
@packages/operator-core/src/operator_core/tui/subprocess.py
@demo/tikv.py
@demo/ratelimiter.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration test infrastructure</name>
  <files>
scripts/integration_tests/__init__.py
scripts/integration_tests/conftest.py
  </files>
  <action>
1. Create the integration_tests directory:
   ```bash
   mkdir -p scripts/integration_tests
   ```

2. Create `scripts/integration_tests/__init__.py`:
   ```python
   """Integration tests for demo end-to-end validation."""
   ```

3. Create `scripts/integration_tests/conftest.py` with shared fixtures:

```python
"""
Shared pytest fixtures for demo integration tests.

These tests require:
- Docker clusters running (TiKV and/or rate limiter depending on test)
- ANTHROPIC_API_KEY environment variable set
- Network access to localhost services

Run with: uv run pytest scripts/integration_tests/ -v
"""

import asyncio
import sys
from pathlib import Path

import pytest

# Add packages to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "packages" / "operator-core" / "src"))
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "packages" / "operator-ratelimiter" / "src"))
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


@pytest.fixture
def subprocess_manager():
    """Create a SubprocessManager for test use."""
    from operator_core.tui.subprocess import SubprocessManager

    mgr = SubprocessManager()
    yield mgr

    # Cleanup: signal shutdown and terminate all
    mgr.shutdown.set()
    # Note: terminate_all is async, but fixture teardown is sync
    # The tests must call terminate_all before exiting


@pytest.fixture
def db_path(tmp_path):
    """Create a temporary database path for test isolation."""
    return tmp_path / "tickets.db"


@pytest.fixture
async def clean_redis():
    """Flush Redis for clean test state."""
    import subprocess as sp

    proc = await asyncio.create_subprocess_exec(
        "docker", "exec", "docker-redis-1", "redis-cli", "FLUSHALL",
        stdout=sp.DEVNULL,
        stderr=sp.DEVNULL,
    )
    await proc.wait()
    yield
    # Flush again after test
    proc = await asyncio.create_subprocess_exec(
        "docker", "exec", "docker-redis-1", "redis-cli", "FLUSHALL",
        stdout=sp.DEVNULL,
        stderr=sp.DEVNULL,
    )
    await proc.wait()
```

Key patterns from research:
- Add packages to sys.path for imports (matches test-demo-flow.py pattern)
- SubprocessManager fixture with cleanup
- Temporary database path for test isolation
- Redis flush fixture for rate limiter tests
  </action>
  <verify>
```bash
cd /Users/jrtipton/x/operator
ls -la scripts/integration_tests/
```
Both __init__.py and conftest.py exist.
  </verify>
  <done>Integration test infrastructure created with shared fixtures</done>
</task>

<task type="auto">
  <name>Task 2: Create TiKV demo integration test</name>
  <files>scripts/integration_tests/test_tikv_demo_e2e.py</files>
  <action>
Create `scripts/integration_tests/test_tikv_demo_e2e.py`:

```python
"""
Integration test for TiKV demo end-to-end flow (TEST-01).

This test verifies:
- Monitor subprocess starts and detects TiKV violations
- Agent subprocess starts and processes tickets
- Subprocess output is captured correctly

Prerequisites:
- TiKV cluster running: docker compose -f docker/tikv/docker-compose.yml up -d
- ANTHROPIC_API_KEY environment variable set

Run with: uv run pytest scripts/integration_tests/test_tikv_demo_e2e.py -v
"""

import asyncio
import os

import pytest


# Skip if ANTHROPIC_API_KEY not set (can't run agent without it)
pytestmark = pytest.mark.skipif(
    "ANTHROPIC_API_KEY" not in os.environ,
    reason="ANTHROPIC_API_KEY required for agent tests"
)


@pytest.mark.asyncio
async def test_tikv_demo_monitor_detects_violations(subprocess_manager, db_path):
    """
    Test that monitor subprocess detects TiKV violations.

    This is a lighter test that doesn't require full agent processing -
    just verifies the monitor can start and detect issues.
    """
    from operator_core.tui.subprocess import SubprocessManager

    mgr = subprocess_manager
    reader_tasks = []

    try:
        # Spawn monitor subprocess
        monitor_proc = await mgr.spawn(
            "monitor",
            [
                "-u", "-m", "operator_core.cli.main",
                "monitor", "run",
                "--subject", "tikv",
                "-i", "3",
                "--db", str(db_path),
            ],
            buffer_size=50,
        )

        # Start reader task
        reader_task = asyncio.create_task(mgr.read_output(monitor_proc))
        reader_tasks.append(reader_task)

        # Wait for monitor to complete at least one check cycle
        await asyncio.sleep(5)

        # Verify monitor is running and producing output
        monitor_buf = mgr.get_buffer("monitor")
        assert monitor_buf is not None, "Monitor buffer should exist"

        output = monitor_buf.get_text()
        assert "Monitor loop starting" in output or "Checking" in output, \
            f"Monitor should show startup or check messages. Got: {output[:200]}"

    finally:
        # Cleanup
        mgr.shutdown.set()
        await mgr.terminate_all()

        for task in reader_tasks:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass


@pytest.mark.asyncio
async def test_tikv_demo_agent_processes_ticket(subprocess_manager, db_path):
    """
    Test that agent subprocess can process a ticket.

    Creates a ticket directly in the database and verifies agent picks it up.
    This avoids needing actual chaos injection.
    """
    from operator_core.tui.subprocess import SubprocessManager
    from operator_core.agent_lab.ticket_ops import TicketOpsDB

    mgr = subprocess_manager
    reader_tasks = []

    try:
        # Create a test ticket directly
        with TicketOpsDB(db_path) as ticket_db:
            ticket_db._conn.execute(
                """
                INSERT INTO tickets (subject_name, invariant_name, message, status)
                VALUES (?, ?, ?, ?)
                """,
                ("tikv", "test-invariant", "Test ticket for integration test", "open"),
            )
            ticket_db._conn.commit()

        # Spawn agent subprocess
        agent_proc = await mgr.spawn(
            "agent",
            ["-u", "-m", "operator_core.agent_lab", str(db_path)],
            buffer_size=100,
            env={
                "OPERATOR_SAFETY_MODE": "execute",
                "ANTHROPIC_API_KEY": os.environ.get("ANTHROPIC_API_KEY", ""),
            },
        )

        # Start reader task
        reader_task = asyncio.create_task(mgr.read_output(agent_proc))
        reader_tasks.append(reader_task)

        # Wait for agent to process (may take several seconds for Claude response)
        await asyncio.sleep(15)

        # Verify agent processed the ticket
        agent_buf = mgr.get_buffer("agent")
        assert agent_buf is not None, "Agent buffer should exist"

        output = agent_buf.get_text()

        # Agent should show startup and ticket processing
        assert "Agent loop starting" in output, \
            f"Agent should show startup message. Got: {output[:200]}"

        # Check for Claude reasoning or tool execution
        has_reasoning = "[Claude]" in output
        has_tool_call = "[Tool Call]" in output
        has_processing = "Processing ticket" in output

        assert has_processing or has_reasoning or has_tool_call, \
            f"Agent should show processing activity. Got: {output[:500]}"

    finally:
        # Cleanup
        mgr.shutdown.set()
        await mgr.terminate_all()

        for task in reader_tasks:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
```

Key patterns from research:
- Skip marker when API key not available
- SubprocessManager from fixture with proper cleanup
- Create test ticket directly (avoids chaos injection complexity)
- Verify subprocess output captured correctly
- Proper async cleanup in finally block
  </action>
  <verify>
```bash
cd /Users/jrtipton/x/operator
uv run python -c "import scripts.integration_tests.test_tikv_demo_e2e; print('Import OK')"
```
Import succeeds without errors.
  </verify>
  <done>TiKV demo integration test created (TEST-01)</done>
</task>

<task type="auto">
  <name>Task 3: Create rate limiter demo integration test</name>
  <files>scripts/integration_tests/test_ratelimiter_demo_e2e.py</files>
  <action>
Create `scripts/integration_tests/test_ratelimiter_demo_e2e.py`:

```python
"""
Integration test for rate limiter demo end-to-end flow (TEST-02).

This test verifies:
- Monitor subprocess starts and detects rate limit violations
- Agent subprocess starts and processes tickets
- Subprocess output is captured correctly

Prerequisites:
- Rate limiter cluster running: docker compose -f docker/docker-compose.yml up -d
- ANTHROPIC_API_KEY environment variable set

Run with: uv run pytest scripts/integration_tests/test_ratelimiter_demo_e2e.py -v
"""

import asyncio
import os

import pytest


# Skip if ANTHROPIC_API_KEY not set (can't run agent without it)
pytestmark = pytest.mark.skipif(
    "ANTHROPIC_API_KEY" not in os.environ,
    reason="ANTHROPIC_API_KEY required for agent tests"
)


@pytest.mark.asyncio
async def test_ratelimiter_demo_monitor_detects_violations(subprocess_manager, db_path, clean_redis):
    """
    Test that monitor subprocess detects rate limiter violations.

    Injects over-limit counters directly into Redis and verifies
    the monitor detects them.
    """
    from operator_core.tui.subprocess import SubprocessManager
    import redis.asyncio as aioredis

    mgr = subprocess_manager
    reader_tasks = []

    try:
        # Inject over-limit counters into Redis
        r = aioredis.from_url("redis://localhost:6379")
        await r.set("ratelimit:test_user_1:counter", "150")  # Over 100 limit
        await r.set("ratelimit:test_user_1:limit", "100")
        await r.close()

        # Spawn monitor subprocess
        monitor_proc = await mgr.spawn(
            "monitor",
            [
                "-u", "-m", "operator_core.cli.main",
                "monitor", "run",
                "--subject", "ratelimiter",
                "-i", "3",
                "--db", str(db_path),
            ],
            buffer_size=50,
        )

        # Start reader task
        reader_task = asyncio.create_task(mgr.read_output(monitor_proc))
        reader_tasks.append(reader_task)

        # Wait for monitor to complete at least one check cycle
        await asyncio.sleep(5)

        # Verify monitor is running and producing output
        monitor_buf = mgr.get_buffer("monitor")
        assert monitor_buf is not None, "Monitor buffer should exist"

        output = monitor_buf.get_text()
        assert "Monitor loop starting" in output or "Checking" in output, \
            f"Monitor should show startup or check messages. Got: {output[:200]}"

    finally:
        # Cleanup
        mgr.shutdown.set()
        await mgr.terminate_all()

        for task in reader_tasks:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass


@pytest.mark.asyncio
async def test_ratelimiter_demo_agent_processes_ticket(subprocess_manager, db_path, clean_redis):
    """
    Test that agent subprocess can process a rate limiter ticket.

    Creates a ticket directly in the database and verifies agent picks it up.
    """
    from operator_core.tui.subprocess import SubprocessManager
    from operator_core.agent_lab.ticket_ops import TicketOpsDB

    mgr = subprocess_manager
    reader_tasks = []

    try:
        # Create a test ticket directly
        with TicketOpsDB(db_path) as ticket_db:
            ticket_db._conn.execute(
                """
                INSERT INTO tickets (subject_name, invariant_name, message, status)
                VALUES (?, ?, ?, ?)
                """,
                (
                    "ratelimiter",
                    "rate-limit-exceeded",
                    "Rate limit exceeded: user test_user_1 at 150/100 requests",
                    "open",
                ),
            )
            ticket_db._conn.commit()

        # Spawn agent subprocess
        agent_proc = await mgr.spawn(
            "agent",
            ["-u", "-m", "operator_core.agent_lab", str(db_path)],
            buffer_size=100,
            env={
                "OPERATOR_SAFETY_MODE": "execute",
                "ANTHROPIC_API_KEY": os.environ.get("ANTHROPIC_API_KEY", ""),
            },
        )

        # Start reader task
        reader_task = asyncio.create_task(mgr.read_output(agent_proc))
        reader_tasks.append(reader_task)

        # Wait for agent to process (may take several seconds for Claude response)
        await asyncio.sleep(15)

        # Verify agent processed the ticket
        agent_buf = mgr.get_buffer("agent")
        assert agent_buf is not None, "Agent buffer should exist"

        output = agent_buf.get_text()

        # Agent should show startup and ticket processing
        assert "Agent loop starting" in output, \
            f"Agent should show startup message. Got: {output[:200]}"

        # Check for Claude reasoning or tool execution
        has_reasoning = "[Claude]" in output
        has_tool_call = "[Tool Call]" in output
        has_processing = "Processing ticket" in output

        assert has_processing or has_reasoning or has_tool_call, \
            f"Agent should show processing activity. Got: {output[:500]}"

    finally:
        # Cleanup
        mgr.shutdown.set()
        await mgr.terminate_all()

        for task in reader_tasks:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass


@pytest.mark.asyncio
async def test_ratelimiter_health_check(clean_redis):
    """
    Smoke test: verify rate limiter API is responding.

    This is a quick sanity check that the cluster is running.
    """
    import httpx

    async with httpx.AsyncClient(timeout=5.0) as client:
        response = await client.get("http://localhost:8001/health")
        assert response.status_code == 200, \
            f"Rate limiter should be healthy. Got: {response.status_code}"

        data = response.json()
        assert data.get("status") == "healthy" or "redis" in data, \
            f"Health response should indicate healthy status. Got: {data}"
```

Key patterns from research:
- clean_redis fixture for test isolation
- Inject test data directly into Redis
- Create test ticket directly in database
- Same cleanup pattern as TiKV test
- Smoke test for cluster health
  </action>
  <verify>
```bash
cd /Users/jrtipton/x/operator
uv run python -c "import scripts.integration_tests.test_ratelimiter_demo_e2e; print('Import OK')"
```
Import succeeds without errors.
  </verify>
  <done>Rate limiter demo integration test created (TEST-02)</done>
</task>

</tasks>

<verification>
Run the integration tests to verify they work:

```bash
cd /Users/jrtipton/x/operator

# List created files
ls -la scripts/integration_tests/

# Run TiKV test (requires TiKV cluster + API key)
uv run pytest scripts/integration_tests/test_tikv_demo_e2e.py -v --tb=short

# Run rate limiter test (requires rate limiter cluster + API key)
uv run pytest scripts/integration_tests/test_ratelimiter_demo_e2e.py -v --tb=short
```

Tests should either pass or skip (if API key not available).
</verification>

<success_criteria>
- TEST-01: TiKV integration test exists and runs (pass or skip)
- TEST-02: Rate limiter integration test exists and runs (pass or skip)
- Test files follow pytest-asyncio patterns from research
- Proper cleanup in all test cases
</success_criteria>

<output>
After completion, create `.planning/phases/34-demo-e2e-validation/34-02-SUMMARY.md`
</output>
