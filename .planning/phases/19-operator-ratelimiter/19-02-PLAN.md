---
phase: 19-operator-ratelimiter
plan: 02
type: execute
wave: 2
depends_on: ["19-01"]
files_modified:
  - packages/operator-ratelimiter/src/operator_ratelimiter/subject.py
  - packages/operator-ratelimiter/src/operator_ratelimiter/invariants.py
autonomous: true

must_haves:
  truths:
    - "RateLimiterSubject.observe() returns dict with nodes, counters, node_metrics, redis_connected"
    - "RateLimiterSubject.reset_counter() calls management API"
    - "RateLimiterInvariantChecker.check() detects node_down violations"
    - "RateLimiterInvariantChecker.check() detects redis_disconnected violations"
    - "RateLimiterInvariantChecker.check() detects high_latency violations after grace period"
    - "RateLimiterInvariantChecker.check() detects counter_drift violations"
    - "RateLimiterInvariantChecker.check() detects ghost_allowing violations"
  artifacts:
    - path: "packages/operator-ratelimiter/src/operator_ratelimiter/subject.py"
      provides: "RateLimiterSubject implementing SubjectProtocol"
      exports: ["RateLimiterSubject"]
      min_lines: 80
    - path: "packages/operator-ratelimiter/src/operator_ratelimiter/invariants.py"
      provides: "RateLimiterInvariantChecker implementing InvariantCheckerProtocol"
      exports: ["RateLimiterInvariantChecker"]
      min_lines: 150
  key_links:
    - from: "subject.py"
      to: "ratelimiter_client.py"
      via: "injected RateLimiterClient for observations"
      pattern: "ratelimiter: RateLimiterClient"
    - from: "subject.py"
      to: "redis_client.py"
      via: "injected RedisClient for state inspection"
      pattern: "redis: RedisClient"
    - from: "invariants.py"
      to: "operator_protocols"
      via: "imports InvariantViolation"
      pattern: "from operator_protocols import InvariantViolation"
---

<objective>
Implement RateLimiterSubject and RateLimiterInvariantChecker following TiKV patterns.

Purpose: Create the core Subject and InvariantChecker implementations that enable MonitorLoop to monitor rate limiter clusters using the same generic code path as TiKV.

Output:
- RateLimiterSubject with observe() returning dict and action methods (reset_counter)
- RateLimiterInvariantChecker with check() detecting 5 invariant types
- Grace period support for latency and drift invariants
</objective>

<execution_context>
@/Users/jrtipton/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jrtipton/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-operator-ratelimiter/19-RESEARCH.md
@.planning/phases/19-operator-ratelimiter/19-01-SUMMARY.md

# Reference implementations to mirror
@packages/operator-tikv/src/operator_tikv/subject.py
@packages/operator-tikv/src/operator_tikv/invariants.py

# Protocol definitions
@packages/operator-protocols/src/operator_protocols/subject.py
@packages/operator-protocols/src/operator_protocols/invariant.py

# Action registry for get_action_definitions()
@packages/operator-core/src/operator_core/actions/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create RateLimiterSubject</name>
  <files>
    packages/operator-ratelimiter/src/operator_ratelimiter/subject.py
  </files>
  <action>
Create RateLimiterSubject implementing SubjectProtocol, mirroring TiKVSubject structure:

1. Create `subject.py` with dataclass `RateLimiterSubject`:
   - Fields:
     - `ratelimiter: RateLimiterClient` (HTTP client for management API)
     - `redis: RedisClient` (Redis client for state inspection)
     - `prom: PrometheusClient` (Prometheus client for metrics)

2. Implement `async def observe() -> dict[str, Any]`:
   - Get nodes from `self.ratelimiter.get_nodes()`
   - Get counters from `self.ratelimiter.get_counters()`
   - Check Redis connectivity: `redis_connected = await self.redis.ping()`
   - Get per-node metrics (latency) for Up nodes:
     ```python
     node_metrics: dict[str, dict[str, Any]] = {}
     for node in nodes:
         if node.state == "Up":
             try:
                 latency_p99 = await self.prom.get_node_latency_p99(node.id)
                 node_metrics[node.id] = {"latency_p99_ms": latency_p99}
             except Exception:
                 pass  # Skip failed metrics
     ```
   - Return observation dict matching TiKV pattern:
     ```python
     return {
         "nodes": [{"id": n.id, "address": n.address, "state": n.state} for n in nodes],
         "counters": [{"key": c.key, "count": c.count, "limit": c.limit, "remaining": c.remaining} for c in counters],
         "node_metrics": node_metrics,
         "redis_connected": redis_connected,
     }
     ```

3. Implement `def get_action_definitions() -> list[ActionDefinition]`:
   - Use ActionDefinition and ParamDef from operator_core.actions.registry
   - Define reset_counter action:
     - name: "reset_counter"
     - description: "Reset rate limit counter for a specific key"
     - parameters: {"key": ParamDef(type="str", description="Rate limit key to reset", required=True)}
     - risk_level: "low"
     - requires_approval: False

4. Implement `async def reset_counter(key: str) -> None`:
   - Fire-and-forget: call `await self.ratelimiter.reset_counter(key)`
   - Do NOT verify the counter was cleared (matches TiKV action semantics)

5. Add docstrings following TiKVSubject style (important for AI reasoning)

6. Do NOT implement update_limit action for now - the ratelimiter-service doesn't have an endpoint for it. Note in docstring that this action is deferred.
  </action>
  <verify>
```bash
cd packages/operator-ratelimiter && python -c "
from operator_ratelimiter.subject import RateLimiterSubject
from operator_protocols import SubjectProtocol

# Verify protocol compliance (runtime_checkable)
assert isinstance(RateLimiterSubject, type)
print('RateLimiterSubject defined')

# Check methods exist
assert hasattr(RateLimiterSubject, 'observe')
assert hasattr(RateLimiterSubject, 'get_action_definitions')
assert hasattr(RateLimiterSubject, 'reset_counter')
print('Required methods exist')
"
```
  </verify>
  <done>RateLimiterSubject implements observe(), get_action_definitions(), and reset_counter() following TiKV patterns</done>
</task>

<task type="auto">
  <name>Task 2: Create RateLimiterInvariantChecker</name>
  <files>
    packages/operator-ratelimiter/src/operator_ratelimiter/invariants.py
  </files>
  <action>
Create RateLimiterInvariantChecker implementing InvariantCheckerProtocol, mirroring TiKVInvariantChecker:

1. Create `invariants.py` with InvariantConfig dataclass (copy from TiKV):
   ```python
   @dataclass
   class InvariantConfig:
       name: str
       grace_period: timedelta = field(default_factory=lambda: timedelta(seconds=0))
       threshold: float = 0.0
       severity: str = "warning"
   ```

2. Define invariant configurations:
   - `NODE_DOWN_CONFIG` - name="node_down", grace_period=0s, severity="critical"
   - `REDIS_DISCONNECTED_CONFIG` - name="redis_disconnected", grace_period=0s, severity="critical"
   - `HIGH_LATENCY_CONFIG` - name="high_latency", grace_period=60s, threshold=50.0 (ms), severity="warning"
   - `COUNTER_DRIFT_CONFIG` - name="counter_drift", grace_period=30s, threshold=0.1 (10% drift), severity="warning"
   - `GHOST_ALLOWING_CONFIG` - name="ghost_allowing", grace_period=0s, severity="critical"

3. Create class `RateLimiterInvariantChecker`:
   - `__init__`: Initialize `self._first_seen: dict[str, datetime] = {}`
   - Copy `_get_violation_key()` and `_check_with_grace_period()` from TiKVInvariantChecker exactly

4. Implement `def check(observation: dict[str, Any]) -> list[InvariantViolation]`:
   - Parse nodes from observation["nodes"]
   - Parse counters from observation["counters"]
   - Parse node_metrics from observation["node_metrics"]
   - Parse redis_connected from observation["redis_connected"]
   - Run all invariant checks, accumulate violations
   - Return violations list

5. Implement individual check methods:

   **check_nodes_up(nodes)** - MON-01:
   - For each node, check if state != "Up"
   - Use grace period (immediate for node_down)
   - Clear tracking for nodes that came back up (copy TiKV pattern)

   **check_redis_connected(redis_connected)** - MON-02:
   - If not redis_connected, report violation
   - Immediate (no grace period)

   **check_latency(node_id, metrics_data)** - MON-03:
   - If latency_p99_ms > threshold (50ms default), report violation
   - 60s grace period to avoid alerting on transient spikes

   **check_counter_drift(counters)** - MON-04:
   - For each counter, check if count > limit * (1 + threshold)
   - This detects if counter exceeds limit (shouldn't happen with atomic Lua)
   - 10% threshold allows for clock skew
   - 30s grace period

   **check_ghost_allowing(counters)** - MON-05:
   - For each counter, check if count > limit
   - This is a simpler form: if counter shows more than limit, ghost allowing occurred
   - Immediate (critical severity)

6. Add `def clear_state()` method to clear _first_seen dict

Note: counter_drift and ghost_allowing have similar detection logic but different semantics:
- counter_drift: Structural issue (count exceeds limit by threshold)
- ghost_allowing: Correctness issue (any count > limit is incorrect)

Keep ghost_allowing check simpler (exact: count > limit) vs drift (count > limit * 1.1)
  </action>
  <verify>
```bash
cd packages/operator-ratelimiter && python -c "
from operator_ratelimiter.invariants import RateLimiterInvariantChecker
from operator_protocols import InvariantCheckerProtocol

# Verify protocol compliance
checker = RateLimiterInvariantChecker()
assert hasattr(checker, 'check')
print('RateLimiterInvariantChecker created')

# Test with empty observation
violations = checker.check({
    'nodes': [],
    'counters': [],
    'node_metrics': {},
    'redis_connected': True,
})
assert isinstance(violations, list)
print('check() returns list')

# Test redis_disconnected detection
violations = checker.check({
    'nodes': [],
    'counters': [],
    'node_metrics': {},
    'redis_connected': False,
})
assert any(v.invariant_name == 'redis_disconnected' for v in violations)
print('Detects redis_disconnected')
"
```
  </verify>
  <done>RateLimiterInvariantChecker implements check() with all 5 invariant types following TiKV patterns</done>
</task>

</tasks>

<verification>
After all tasks complete:

```bash
# Verify subject and checker work together
cd packages/operator-ratelimiter && python -c "
from operator_ratelimiter.subject import RateLimiterSubject
from operator_ratelimiter.invariants import RateLimiterInvariantChecker

# Verify they can be instantiated (with mock clients)
checker = RateLimiterInvariantChecker()

# Test observation structure
test_observation = {
    'nodes': [{'id': 'node1', 'address': 'localhost:8080', 'state': 'Up'}],
    'counters': [{'key': 'test', 'count': 5, 'limit': 10, 'remaining': 5}],
    'node_metrics': {'node1': {'latency_p99_ms': 10.0}},
    'redis_connected': True,
}

violations = checker.check(test_observation)
print(f'Violations found: {len(violations)}')

# Test node down detection
test_observation['nodes'][0]['state'] = 'Down'
violations = checker.check(test_observation)
assert any(v.invariant_name == 'node_down' for v in violations)
print('Node down detection works')
"

# Verify imports in __init__.py are updated in Plan 04
```
</verification>

<success_criteria>
- RateLimiterSubject.observe() returns dict with nodes, counters, node_metrics, redis_connected
- RateLimiterSubject.get_action_definitions() returns ActionDefinition for reset_counter
- RateLimiterInvariantChecker.check() correctly detects all 5 invariant types
- Grace period logic works (latency, counter_drift have grace periods; others are immediate)
- Code structure mirrors TiKV implementation for consistency
</success_criteria>

<output>
After completion, create `.planning/phases/19-operator-ratelimiter/19-02-SUMMARY.md`
</output>
